INFORMATION: dropout_ratio=0.2 minibatch-size=128 filter-nb=150 filt_len=2 pool_len=3 vocab=90
----------------------------------------------------------------------
----------------------------------------------------------------------
INFORMATION: dropout_ratio=0.2 minibatch-size=128 filter-nb=150 filt_len=2 pool_len=3 vocab=90
----------------------------------------------------------------------
Loading data...
./data/nn_data/dev.csv
./data/nn_data/test.csv
./data/nn_data/train.csv
Nb of tweets: train: 1712 test: 484 dev: 242
Total vocabulary size: 5770
Pruned vocabulary size: 90.0% =5193
('Random seed', '113')
Loading pre-trained word2vec embeddings......
 Number of words found in emb matrix: 3966 of 5193
............................
1712 train tweets
484 test  tweets
242 dev   tweets
5193 vocabulary size
6 different classes
............................
** optimizer: adadelta
Building model: convolutional neural network (cnn)
Doing classification with class # 6
Training and validating ....
Train on 1712 samples, validate on 242 samples
Epoch 1/25
Epoch 00000: val_loss improved from inf to 1.55414, saving model to saved_models/cnn-adadelta-150-2-3-True-categorical_crossentropy-128-0.2-init-pretrained-90.0-128-128.model.cl.6.dom.in
6s - loss: 1.7020 - val_loss: 1.5541
Epoch 2/25
Epoch 00001: val_loss improved from 1.55414 to 1.34444, saving model to saved_models/cnn-adadelta-150-2-3-True-categorical_crossentropy-128-0.2-init-pretrained-90.0-128-128.model.cl.6.dom.in
4s - loss: 1.5240 - val_loss: 1.3444
Epoch 3/25
Epoch 00002: val_loss improved from 1.34444 to 1.23633, saving model to saved_models/cnn-adadelta-150-2-3-True-categorical_crossentropy-128-0.2-init-pretrained-90.0-128-128.model.cl.6.dom.in
4s - loss: 1.3076 - val_loss: 1.2363
Epoch 4/25
Epoch 00003: val_loss improved from 1.23633 to 1.06244, saving model to saved_models/cnn-adadelta-150-2-3-True-categorical_crossentropy-128-0.2-init-pretrained-90.0-128-128.model.cl.6.dom.in
4s - loss: 1.2085 - val_loss: 1.0624
Epoch 5/25
Epoch 00004: val_loss did not improve
4s - loss: 1.0276 - val_loss: 1.2683
Epoch 6/25
Epoch 00005: val_loss did not improve
4s - loss: 1.0121 - val_loss: 1.0973
Epoch 7/25
Epoch 00006: val_loss did not improve
5s - loss: 0.9614 - val_loss: 1.0672
Epoch 8/25
Epoch 00007: val_loss improved from 1.06244 to 0.87956, saving model to saved_models/cnn-adadelta-150-2-3-True-categorical_crossentropy-128-0.2-init-pretrained-90.0-128-128.model.cl.6.dom.in
4s - loss: 0.8594 - val_loss: 0.8796
Epoch 9/25
Epoch 00008: val_loss did not improve
5s - loss: 0.7893 - val_loss: 0.9453
Epoch 10/25
Epoch 00009: val_loss did not improve
5s - loss: 0.7691 - val_loss: 0.9986
Epoch 11/25
Epoch 00010: val_loss did not improve
4s - loss: 0.7289 - val_loss: 0.9289
Epoch 12/25
Epoch 00011: val_loss did not improve
5s - loss: 0.6684 - val_loss: 0.9406
Epoch 00011: early stopping
Loading ... saved_models/cnn-adadelta-150-2-3-True-categorical_crossentropy-128-0.2-init-pretrained-90.0-128-128.model.cl.6.dom.in
Test model ...
 32/484 [>.............................] - ETA: 2s128/484 [======>.......................] - ETA: 0s224/484 [============>.................] - ETA: 0s320/484 [==================>...........] - ETA: 0s384/484 [======================>.......] - ETA: 0s448/484 [==========================>...] - ETA: 0sRaw Accuracy: 0.6735537190082644
                              precision    recall  f1-score   support

        Affected individuals     0.7111    0.6275    0.6667        51
  Donations and volunteering     0.6889    0.8052    0.7425        77
Infrastructure and utilities     0.8333    0.2174    0.3448        46
   Not related or irrelevant     0.7216    0.8881    0.7962       143
    Other Useful Information     0.5839    0.6444    0.6127       135
        Sympathy and support     0.6667    0.2500    0.3636        32

                   micro avg     0.6736    0.6736    0.6736       484
                   macro avg     0.7009    0.5721    0.5878       484
                weighted avg     0.6839    0.6736    0.6513       484

Confusion Matrix:
 [[ 32   6   1   4   7   1]
 [  1  62   0   2  11   1]
 [  5   4  10   2  24   1]
 [  1   2   0 127  13   0]
 [  5  11   1  30  87   1]
 [  1   5   0  11   7   8]]
 micro pre: 0.6735537190082644 rec: 0.6735537190082644 f-score: 0.6735537190082644
 macro pre: 0.7009139210900956 rec: 0.5720989037485201 f-score: 0.5877599812369904
----------------------------------------------------------------------
